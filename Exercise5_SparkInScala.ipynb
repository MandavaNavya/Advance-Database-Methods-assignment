{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 5 (Spark in Scala)   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     [4 points]\n",
    "---\n",
    "\n",
    "For this exercise, you will work on this JupyterLab notebook, and solve the tasks listed herein. These tasks, in addition to writing Spark code, require you to analyse various query plans and to reason about them.\n",
    "\n",
    "To familiarise yourself with Spark and the Scala language, we also provide you with two JupyterLab notebooks,which you can upload on JupyterLab and run yourself. To get a deeper understanding, and look up the types and definitions of various functions, we recommend that you visit the Spark and Spark SQL documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) From SQL to Dataframe (and back again)\n",
    "\n",
    "#### Find for each of the Spark SQL queries an equivalent one that only uses the Dataframe API (or vice versa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// val badgesDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/badges.csv\")\n",
    "val badgesDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/badges.csv\")\n",
    "val commentsDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/comments.csv\")\n",
    "val postsDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/posts.csv\")\n",
    "val postlinksDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/postlinks.csv\")\n",
    "val usersDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/users.csv\")\n",
    "val votesDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/user/adbs23_shared/stackexchange/votes.csv\")\n",
    "\n",
    "\n",
    "// Creating the views for SparkSQL\n",
    "badgesDF.createOrReplaceTempView(\"badges\")\n",
    "commentsDF.createOrReplaceTempView(\"comments\")\n",
    "postsDF.createOrReplaceTempView(\"posts\")\n",
    "postlinksDF.createOrReplaceTempView(\"postlinks\")\n",
    "usersDF.createOrReplaceTempView(\"users\")\n",
    "votesDF.createOrReplaceTempView(\"votes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badgesDF.printSchema()\n",
    "commentsDF.printSchema()\n",
    "postsDF.printSchema()\n",
    "postlinksDF.printSchema()\n",
    "usersDF.printSchema()\n",
    "votesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query1 = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) FROM comments as c, votes as v, users as u\n",
    "WHERE u.Id = c.UserId AND u.Id = v.UserId\n",
    "AND v.BountyAmount<=100 AND u.UpVotes=0;\n",
    "\"\"\")\n",
    "query1.show()\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query1DF = \n",
    "query1DF.show()\n",
    "query1DF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query2 = usersDF.filter(\"not isnull(location)\").groupBy(\"location\", \"creationdate\").agg(count(\"*\").as(\"cnt\")).orderBy(desc(\"cnt\")).limit(5)\n",
    "query2.show(false)\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query2SQL = spark.sql(\"\")\n",
    "query2SQL.show(false)\n",
    "query2SQL.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3: Transform the given Dataframe API query into the dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "val query = \"\"\"\n",
    "SELECT COUNT(*) FROM\n",
    "posts p, users u, badges b, votes v, postLinks pl\n",
    "WHERE b.UserId = u.Id AND p.OwnerUserId = u.Id\n",
    "AND u.Id = v.UserId AND p.Id = pl.RelatedPostId\n",
    "AND p.Score>=0;\n",
    "\"\"\"\n",
    "\n",
    "val query3 = spark.sql(query)\n",
    "query3.show(false)\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query3DF = \n",
    "query3.show(false)\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- ### **b) Analyzing Spark SQL Plans**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that queries such as query 3, which follow a simple but common structure are not solved optimally in state-of-the-art DBMSs[1][2]. This is due to the fact that the systems execute the query as a sequence of natural joins, producing more intermediate results than necessary. By rewriting the query in a way which propagates only the counts upwards without materializing all tuples, we can avoid a significant overhead.\n",
    "\n",
    "The following query, query3_opt is a rewritten version of query3, which results in a lower runtime while being semantically equivalent.\n",
    "\n",
    "Investigate and compare the query plans of query3 and query3_opt.\n",
    "\n",
    "1) Which physical operators does Spark SQL use to realize the join operations?\n",
    "\n",
    "2) How are the physical join operators split up into codegen stages in these plans and why is it desirable to have few large codegen stages rather than many small ones?\n",
    "\n",
    "3) How many rows result as intermediate outputs in total during the execution of query3 and of query3_opt?\n",
    "\n",
    "You can use the Spark SQL web UI to find the execution graphs.\n",
    "\n",
    "[1] Georg Gottlob, Matthias Lanzinger, Davide Mario Longo, Cem Okulmus, Reinhard\n",
    "Pichler, and Alexander Selzer. 2023. Structure-Guided Query Evaluation: Towards\n",
    "Bridging the Gap from Theory to Practice. CoRR abs/2303.02723 (2023).\n",
    "https://doi.org/10.48550/arXiv.2303.02723 arXiv:2303.02723\n",
    "\n",
    "[2] Reinhard Pichler and Sebastian Skritek. 2013. Tractable counting of the answers\n",
    "to conjunctive queries. J. Comput. Syst. Sci. 79, 6 (2013), 984â€“1001.\n",
    "https://doi.org/10.1016/j.jcss.2013.01.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "val query_opt = \"\"\"\n",
    "SELECT SUM(c_pl_p_u_b * c_v) AS cnt FROM\n",
    "    (SELECT UserId, 1 AS c_v FROM votes) v,\n",
    "    (SELECT pl_p_u.Id, SUM(c_pl_p_u * c_b) AS c_pl_p_u_b FROM\n",
    "        (SELECT UserId, 1 AS c_b FROM badges) b,\n",
    "        (SELECT u.Id, SUM(c_pl_p * c_u) AS c_pl_p_u FROM\n",
    "            (SELECT Id, 1 AS c_u FROM users) u,\n",
    "            (SELECT OwnerUserId, SUM(c_pl * c_p) AS c_pl_p FROM\n",
    "                (SELECT RelatedPostId, 1 AS c_pl FROM postlinks) pl,\n",
    "                (SELECT OwnerUserId, Id, 1 AS c_p FROM posts WHERE Score>=0) p\n",
    "                WHERE p.Id = pl.RelatedPostId\n",
    "                GROUP BY OwnerUserId) pl_p\n",
    "        WHERE u.Id = pl_p.OwnerUserId\n",
    "        GROUP BY u.Id) AS pl_p_u\n",
    "    WHERE pl_p_u.Id = b.UserId\n",
    "    GROUP BY pl_p_u.Id) pl_p_u_b\n",
    "    WHERE pl_p_u_b.Id = v.UserId\n",
    "\"\"\"\n",
    "\n",
    "val query3_opt = spark.sql(query_opt)\n",
    "query3_opt.show(false) // 'false' turns of truncation of row entries\n",
    "query3_opt.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
